{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b2b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "sql_conn = pyodbc.connect(\n",
    "    r\"DRIVER={ODBC Driver 18 for SQL Server};\"\n",
    "    r\"SERVER=DESKTOP-MS4DILS\\THUS;\"\n",
    "    r\"DATABASE=US_Traffic_Accidents_ETL;\"\n",
    "    r\"Trusted_Connection=yes;\"\n",
    "    r\"Encrypt=yes;\"\n",
    "    r\"TrustServerCertificate=yes;\",\n",
    "    autocommit=True\n",
    ")\n",
    "\n",
    "#allows interaction with sql database\n",
    "cursor = sql_conn.cursor()\n",
    "\n",
    "#test connection\n",
    "print(sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf80541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map pandas dtypes to SQL Server types\n",
    "def map_dtype(dtype, max_val=None):\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        if max_val is not None:\n",
    "            return 'BIGINT' if max_val > 2_147_483_647 else 'INT'\n",
    "        return 'BIGINT'\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return \"FLOAT\"\n",
    "    elif pd.api.types.is_bool_dtype(dtype):\n",
    "        return \"BIT\"\n",
    "    elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "        return \"DATETIME\"\n",
    "    else:\n",
    "        return \"NVARCHAR(MAX)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d57c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this section if grabbing data from sql server into postgresql\n",
    "# tables = ['DimProduct', 'DimProductSubcategory', 'DimProductCategory', 'DimSalesTerritory', 'FactInternetSales']\n",
    "\n",
    "# #Loop through wanted tables from SQL Server and load into PostgreSQL extract location\n",
    "# for table in tables:\n",
    "#     query = f\"SELECT * FROM {table}\"\n",
    "#     df = pd.read_sql(query, sql_conn)\n",
    "    \n",
    "#     # Load into PostgreSQL\n",
    "#     df.to_sql(table.lower(), pg_engine, index=False, if_exists='replace')  # Use lower case for PostgreSQL\n",
    "#     print(f\"Loaded {table} into PostgreSQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22beb05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the maximum number of rows to display\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Increase the maximum number of columns to display\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# If necessary, increase the maximum column width\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#removes white space function\n",
    "def remove_whitespace(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            df[column] = df[column].str.strip()\n",
    "    return df\n",
    "\n",
    "#turns csv into sql server extract table\n",
    "def csv_to_extract(df, table_name):\n",
    "    cursor = sql_conn.cursor()\n",
    "    \n",
    "    columns_with_types = ', '.join(\n",
    "    f\"{col} {map_dtype(dtype)}\" for col, dtype in df.dtypes.items()\n",
    "    )\n",
    "    drop_stmt = f\"DROP TABLE IF EXISTS {table_name}\"\n",
    "    cursor.execute(drop_stmt)\n",
    "    create_stmt = f\"IF OBJECT_ID('{table_name}', 'U') IS NULL CREATE TABLE {table_name} ({columns_with_types})\" \n",
    "    cursor.execute(create_stmt)\n",
    "    sql_conn.commit()\n",
    "    # Insert data into table\n",
    "    for _, row in df.iterrows():\n",
    "        placeholders = ', '.join('?' for _ in row)\n",
    "        insert_stmt = f\"INSERT INTO {table_name} VALUES ({placeholders})\"\n",
    "        cursor.execute(insert_stmt, *row)\n",
    "    sql_conn.commit()\n",
    "    cursor.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45fa69",
   "metadata": {},
   "source": [
    "Accidents 2020 -> Transformation Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aaf7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accidents 2020\n",
    "filepath = r'acc_20.csv'\n",
    "data = pd.read_csv(filepath,encoding='cp1252')\n",
    "acc_20 = data.copy()\n",
    "\n",
    "#check nulls\n",
    "    #Only work zone has nulls but we will be leaving it as-is\n",
    "#remove whitespace\n",
    "acc_20 = remove_whitespace(acc_20)\n",
    "#standardize data (if any)\n",
    "    #no changes\n",
    "#remove duplicates (if any)\n",
    "acc_20.drop_duplicates()\n",
    "#data type conversions (if any)\n",
    "    #no changes\n",
    "#create a new csv file (this is our tranformation table)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a222b334",
   "metadata": {},
   "source": [
    "Accidents: 2016, 2017, 2018 --> Transfomation Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #accidents 2016\n",
    "# filepath = r'acc_16.csv'\n",
    "# data = pd.read_csv(filepath,encoding='cp1252')\n",
    "# acc_16 = data.copy()\n",
    "# #accidents 2017\n",
    "# filepath = r'acc_17.csv'\n",
    "# data = pd.read_csv(filepath,encoding='cp1252')\n",
    "# acc_17= data.copy()\n",
    "# #accidents 2018\n",
    "# filepath = r'acc_18.csv'\n",
    "# data = pd.read_csv(filepath,encoding='cp1252')\n",
    "# acc_18 = data.copy()\n",
    "# #accidents 2019\n",
    "# filepath = r'acc_19.csv'\n",
    "# data = pd.read_csv(filepath,encoding='cp1252')\n",
    "# acc_19 = data.copy()\n",
    "# #drop columns\n",
    "# columns_to_drop = ['WEATHER2','CF1', 'CF2', 'CF3']\n",
    "# acc_16 = acc_16.drop(columns=columns_to_drop)\n",
    "# acc_17 = acc_17.drop(columns=columns_to_drop)\n",
    "# acc_18 = acc_18.drop(columns=columns_to_drop)\n",
    "# columns_to_drop = ['WEATHER1', 'WEATHER2', 'CF1', 'CF1NAME', 'CF2', 'CF2NAME', 'CF3', 'CF3NAME']\n",
    "# acc_19 = acc_19.drop(columns=columns_to_drop)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
